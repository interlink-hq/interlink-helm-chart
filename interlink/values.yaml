# Default values for interlink.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nodeName: virtual-node

global:
  images:
    # -- imagePullPolicy to apply to all containers
    pullPolicy: Always
    # -- Secrets with credentials to pull images from a private registry
    # Eg: pullSecrets: [ name: pull-secret1, name: pull-secret2 ]
    pullSecrets: []

interlink:
  image:
    registry: "ghcr.io/"
    repository: "interlink-hq/interlink/interlink"
    tag: "latest"
  enabled: false
  exportPodData: false
  dataRootVolume: ""
  # unix socket in /var/run
  socket: null
  # if socket is specified, the address/port are ignored
  address: http://localhost
  port: 4000
  kubernetesApiAddr: ""
  kubernetesApiPort: ""
  kubernetesApiCaCrt: ""
  # If true, do nothing about projected volumes (flag useful in test environment)
  disableProjectedVolumes: false
  # TLS configuration for secure communication
  tls:
    enabled: false
    certFile: ""
    keyFile: ""
    caCertFile: ""
  # Logging configuration
  logging:
    verboseLogging: false
    errorsOnlyLogging: false
  # Tracing configuration
  tracing:
    enabled: false
  # Plugin endpoint configuration for VPN/networking
  pluginEndpoint:
    url: ""
    port: ""
  # Extra volume mounts for the interlink container
  extraVolumeMounts: []
    # - name: data-root
    #   mountPath: /data
    #   readOnly: false
  # # Enable job script translation from outside service
  # jobScript:
  #   JobScriptBuildConfig:
  #     singularity_hub: {}
  #     apptainer_options: {}
  #     volume_options: {}

interlinkTestConnection:
  image:
    registry: "docker.io/"
    repository: "bitnami/kubectl"
    tag: "latest"

virtualNode:
  image:
    registry: "ghcr.io/"
    repository: "interlink-hq/interlink/virtual-kubelet-inttw"
    tag: "latest"
  resources:
    CPUs: 8
    memGiB: 49
    pods: 100
    # add accelerators to the virtual node
    # accelerators:
    # fpga example
    # - resourceType: xilinx.com/fpga
    #  model: u55c
    #  available: 1
    # gpu example
    # - resourceType: nvidia.com/gpu
    #  model: a100
    #  available: 1
  HTTPProxies:
    HTTP: null
    HTTPs: null
  HTTP:
    insecure: true
    CACert: ""
  kubeletHTTP:
    insecure: true
  # Tracing configuration
  tracing:
    enabled: false
  # uncomment to enable custom nodeSelector and nodeTaints
  # nodeLabels:
  #  - "accelerator=a100"
  # nodeTaints:
  #  - key: "accelerator"
  #    value: "a100"
  #    effect: "NoSchedule"
  # Extra volume mounts for the virtual kubelet container
  extraVolumeMounts: []
    # - name: data-root
    #   mountPath: /data
    #   readOnly: false

  # # Indicate endpoint for JobScript translator
  # JobScriptBuilderURL: "https://test.translator.com/podtranslate"

  # Network tunnel configuration for wstunnel
  network:
    # Enable tunnel feature (creates wstunnel template ConfigMap)
    enableTunnel: false
    # Container image for wstunnel
    tunnelImage:
      registry: "ghcr.io/"
      repository: "erebe/wstunnel"
      tag: "latest"
    # DNS domain for ingress (e.g., "example.com")
    wildcardDNS: ""
    # Path where wstunnel template will be mounted in VK container
    wstunnelTemplatePath: "/etc/templates/wstunnel.yaml"
    # Template type: "nginx" (default) or "traefik"
    templateType: ""
    # Custom wstunnel template content (optional, overrides templateType if set)
    customTemplate: ""
    wstunnelCommand: |
      curl -L https://github.com/erebe/wstunnel/releases/download/v10.4.4/wstunnel_10.4.4_linux_amd64.tar.gz -o wstunnel.tar.gz && tar -xzvf wstunnel.tar.gz && chmod +x wstunnel\\n\\n./wstunnel client --http-upgrade-path-prefix %s %s ws://%s:80

plugin:
  enabled: false
  image:
    registry: ""
    repository: ""
    tag: ""
    # if defined, overrides global.images.pullPolicy
    pullPolicy: null
  command: []
  args: []
  # content of the "config" will be mounted as /etc/interlink/plugin.yaml
  config: ""
  envs: []
    # - name: SLURMCONFIGPATH
    #   value: /etc/interlink/plugin.yaml
  socket: null
  # if socket is specified, the address/port are ignored
  address: ""
  port: 3000
  # Run plugin container with privileged permissions
  privileged: false
  # Extra volume mounts for the plugin container
  extraVolumeMounts: []
    # - name: data-root
    #   mountPath: /data
    #   readOnly: false

sshBastion:
  enabled: false
  image:
    registry: "lscr.io/"
    repository: "linuxserver/openssh-server"
    tag: "latest"
  clientKeys:
    authorizedKeys: ""
    keysURL: ""
  hostKeys:
    priv: ""
    pub: ""
  port: 31022
  # Extra volume mounts for the SSH bastion container
  extraVolumeMounts: []
    # - name: data-root
    #   mountPath: /data
    #   readOnly: false
# disable OAUTH when using sockets for communication in the in-cluster mode
OAUTH:
  enabled: false
  image:
    registry: "ghcr.io/"
    repository: "interlink-hq/interlink/virtual-kubelet-inttw-refresh"
    tag: "latest"
  TokenURL: DUMMY
  ClientID: DUMMY
  ClientSecret: DUMMY
  RefreshToken: DUMMY
  GrantType: authorization_code
  Audience: DUMMY
  # Extra volume mounts for the OAuth refresh token container
  extraVolumeMounts: []
    # - name: data-root
    #   mountPath: /data
    #   readOnly: false

# define extra volumes (e.g. for DataRootVolume)
extraVolumes: []
  # - name: data-root
  #   hostPath:
  #     path: /scratch/interlink/
  #     type: DirectoryOrCreate

# Pod scheduling configuration
podScheduling:
  # Node selector for the interlink pod
  nodeSelector: {}
    # kubernetes.io/hostname: specific-node
    # node-type: worker

  # Tolerations for the interlink pod
  tolerations: []
    # - key: "node-type"
    #   operator: "Equal"
    #   value: "gpu"
    #   effect: "NoSchedule"
    # - key: "dedicated"
    #   operator: "Exists"
    #   effect: "NoSchedule"
